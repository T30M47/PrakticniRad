Za pokrenuti spark ETL skripte (paziti da ste u root folderu - prakticnirad):
python Apache\ Spark/run_scripts.py 

Nakon pokretanja svih kontejnera:
za spajanje na transakcijske baze podataka u novom terminalu:
docker exec -it postgres_1 psql -U postgres -d transakcije
docker exec -it postgres_3 psql -U postgres -d skladiste

za spajanje na skladiste u novom terminalu:
docker exec -it postgres_2 psql -U postgres -d warehouse

za pregled dostupnih tablica nakon spajanje na bazu ili skladište u terminalu napišite:
\dt

za pregled informacija o poojedinoj tablici u terminalu napišite:
\d+ naziv_tablice

kada se spojite na warehouse, možete isprobati sljedeće upite:

ispis prosječne količine i iznosa prodanih proizvoda po trgovinama u 3. mjesecu svih godina:
SELECT
    t.id_trgovine,
    tr.naziv_trgovine AS store_name,
    ROUND(AVG(t.kolicina), 2) AS avg_quantity_sold,
    ROUND(AVG(t.ukupna_cijena::numeric), 2) AS avg_price
FROM
    transakcije t
JOIN
    trgovine tr ON t.id_trgovine = tr.id_trgovine
JOIN
    vrijeme v ON t.id_vrijeme = v.id_vrijeme
WHERE
    v.month = 3
GROUP BY
    t.id_trgovine, store_name;

prikaz ukupne količine i ukupnog iznosa transakcija po mjesecima

SELECT
    v.month,
    ROUND(SUM(t.kolicina),2) AS total_amount_sold,
    ROUND(SUM(t.ukupna_cijena::numeric), 2) as total_amount
FROM                                    
    transakcije t
JOIN             
    vrijeme v ON t.id_vrijeme = v.id_vrijeme
GROUP BY                                    
    v.month
ORDER BY
    v.month;


prikaz najčešće prodavaniih proizvoda po trgovinama u 2022. godini

SELECT
    p.barkod_id,
    p.naziv_proizvoda,
    ROUND(SUM(t.kolicina), 2) AS total_quantity_sold,
    tr.naziv_trgovine,
    tr.lokacija,
    v.year
FROM
    transakcije t
JOIN 
    trgovine tr ON t.id_trgovine = tr.id_trgovine
JOIN 
    vrijeme v ON t.id_vrijeme = v.id_vrijeme
JOIN
    proizvodi p ON t.barkod_id = p.barkod_id
WHERE
    v.year = 2022
GROUP BY
    p.barkod_id, p.naziv_proizvoda, v.year, tr.naziv_trgovine, tr.lokacija
ORDER BY
    total_quantity_sold DESC, v.year;






















































docker exec -it 11303d0511d7 spark-submit   --master local[*]   --driver-class-path /app/postgresql-42.7.1.jar  /app/'Apache Spark'/spark_session.py

docker exec -it a80bb5e16f48 spark-submit   --master local[*]   --driver-class-path /app/postgresql-42.7.1.jar  /app/'Apache Spark'/spark_proizvodi.py
docker exec -it 53a1f8d43309 spark-submit   --master local[*]   --driver-class-path /app/postgresql-42.7.1.jar  /app/'Apache Spark'/spark_trgovine.py
docker exec -it 53a1f8d43309 spark-submit   --master local[*]   --driver-class-path /app/postgresql-42.7.1.jar  /app/'Apache Spark'/spark_vrijeme.py
docker exec -it 53a1f8d43309 spark-submit   --master local[*]   --driver-class-path /app/postgresql-42.7.1.jar  /app/'Apache Spark'/spark_transakcije.py


























SELECT constraint_name, constraint_type
FROM information_schema.table_constraints
WHERE table_name = 'Proizvodi';









































docker system prune


spark = SparkSession.builder \
    .appName("OrphanedRecords") \
    .getOrCreate()

# Load Transakcije, Proizvodi, and Trgovine tables into Spark DataFrames
transakcije_df = spark.read.jdbc(url=database_url, table="Transakcije", properties=database_properties)
proizvodi_df = spark.read.jdbc(url=database_url, table="Proizvodi", properties=database_properties)
trgovine_df = spark.read.jdbc(url=database_url, table="Trgovine", properties=database_properties)

# Identify duplicated rows in Proizvodi and Trgovine
valid_proizvod_id = proizvodi_df.groupBy("barkod_id").agg(F.first("barkod_id").alias("ValidProizvodiID"))
valid_trgovina_id = trgovine_df.groupBy("id_trgovine").agg(F.first("id_trgovine").alias("ValidTrgovinaID"))

# Identify valid values from the remaining rows
#valid_proizvod_id = proizvodi_df.join(duplicated_proizvodi_df, "barkod_id", "left_anti").select("ProizvodID").first()["ProizvodID"]
#valid_trgovina_id = trgovine_df.join(duplicated_trgovine_df, "id_trgovine", "left_anti").select("id_trgovine").first()["id_trgovine"]

# Identify orphaned records
orphaned_transakcije_df = transakcije_df.filter(~transakcije_df["barkod_id"].isin(proizvodi_df.select("barkod_id").rdd.flatMap(lambda x: x).collect()) |
                                                 ~transakcije_df["id_trgovine"].isin(trgovine_df.select("id_trgovine").rdd.flatMap(lambda x: x).collect()))

orphaned_transakcije_df = orphaned_transakcije_df.join(valid_proizvod_id, "barkod_id", "left").join(valid_trgovina_id, "id_trgovine", "left")

# Show or save the updated orphaned records
orphaned_transakcije_df.show()

table_name = "Transakcije"

df_transformed.write.jdbc(url=database_url, table=table_name, mode="overwrite", properties=database_properties)

#df_transformed.write.jdbc(url=database_url, table=new_table_name, mode="overwrite", properties=database_properties)

# Step 6: Execute ETL Job
spark.stop()
